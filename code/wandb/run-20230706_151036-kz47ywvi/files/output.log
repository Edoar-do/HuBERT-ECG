[32m2023-07-06 15:10:38.529[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m33[39m - [1mEmbedder type conv
[32m2023-07-06 15:10:38.529[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m34[39m - [1mPositional encoding 1d
[32m2023-07-06 15:10:38.529[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m35[39m - [1mDecodying type rev
[32m2023-07-06 15:10:38.530[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m43[39m - [1mcreating datasets for self-supervised pre-training...
[32m2023-07-06 15:10:42.170[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m50[39m - [1mcreating dataloaders to generata batches...
[32m2023-07-06 15:10:42.172[39m | [1mINFO    [22m | [36m__main__[39m:[36mtrain[39m:[36m60[39m - [1mcreating model...
Traceback (most recent call last):
  File "Run.py", line 154, in <module>
    train()
  File "Run.py", line 64, in train
    model = FullModel(embedding_type=wandb.config['embedding_type'],
  File "/data/ECG_AF/ECG_pretraining/code/Model.py", line 457, in __init__
    self.transformer_encoder = TransfomerEncoder(d_model, n_heads, dim_ff, p_dropout, n_encoding_layers)
  File "/data/ECG_AF/ECG_pretraining/code/Model.py", line 178, in __init__
    self.encoder_layer = nn.TransformerEncoderLayer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py", line 445, in __init__
    self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py", line 969, in __init__
    assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
AssertionError: embed_dim must be divisible by num_heads